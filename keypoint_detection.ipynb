{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RIPTelefon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QReVnPfPNLxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchvision.models.detection.keypoint_rcnn import KeypointRCNNPredictor\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "drive.mount('/drive')\n",
        "drive_base_path = \"/drive/My Drive/\"\n",
        "root_dir = drive_base_path + t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyXJ3aL32Ow3",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf4-cz3lD4RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Method to draw annotated keypoints on the corresponding image.\n",
        "A helper method to visualize the data, not necessary to the model.\n",
        "'''\n",
        "def show_hand_keypoints(image, hand_keypoints):\n",
        "    \"\"\"Show image with hand keypoints\"\"\"\n",
        "    plt.figure(figsize = (30, 8.75))\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(hand_keypoints[:, 0], hand_keypoints[:, 1], s=10, marker='.', c='r')\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxbNWToBMozX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A trial method to draw calculated hand box on the corresponding image.\n",
        "Used to test our hand box success, not necessary to the model.\n",
        "'''\n",
        "def draw_hand_box(image, box):\n",
        "  \"\"\"Show image with hand box\"\"\"\n",
        "  fig,ax = plt.subplots(1)\n",
        "  fig.set_size_inches(15, 10, forward=True)\n",
        "  ax.imshow(image)\n",
        "  rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w_xuizJ28LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Normalizes the image of size (C x H x W)\n",
        "'''\n",
        "def normalizeImage(image):\n",
        "    image = image / 255.\n",
        "    image = image -np.array([0.485, 0.456, 0.406])[:,None,None]\n",
        "    image = image / np.array([0.229, 0.224, 0.225])[:,None,None]\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsJkvf1aXC6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Since the model expects the box of the interest object and our dataset does not \n",
        "contain hand box but instead the hand center, we calculate the approximate\n",
        "hand box here.\n",
        "'''\n",
        "def get_hand_box(hand_keypoints):\n",
        "  mins, maxs = np.amin(hand_keypoints, axis=0), np.amax(hand_keypoints, axis=0)\n",
        "  x1, x2, y1, y2 = mins[0], maxs[0], mins[1], maxs[1]\n",
        "  marginx = (x2 - x1) * 0.15 # 0.15 * box_width margin\n",
        "  marginy = (y2 - y1) * 0.15 # 0.15 * box_height margin\n",
        "  return(np.asarray([int(x1 - marginx), int(y1 - marginy) , int(x2 + marginx), int(y2 + marginy)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-e_umMRhCWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "To load dataset into the model, we need the images in each batch \n",
        "have the same size. This rescaler is passed to HandKeypointDataset.\n",
        "'''\n",
        "class Rescale(object):\n",
        "    '''Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, hand_keypoints, hand_box, label = sample['image'], sample['keypoints'], sample['boxes'], sample['labels']\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        rescaled_image = transform.resize(image, (new_h, new_w)) # Rescale image\n",
        "\n",
        "        # h and w are swapped for landmarks because for images,\n",
        "        # x and y axes are axis 1 and 0 respectively\n",
        "        for i in range(len(hand_keypoints)):\n",
        "          hand_keypoints[i] = hand_keypoints[i] * [new_h / h, new_w / w, 1] # Rescale hand keypoints as well\n",
        "        hand_box = (hand_box * [new_h / h,  new_w / w, new_h / h, new_w / w]).astype(int) # Rescale hand box as well\n",
        "        \n",
        "       \n",
        "        return {'image': rescaled_image, 'keypoints': hand_keypoints, 'boxes': hand_box, 'labels': label}\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qDH08PSZ4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "When rescale object is used, only one dimention (height or width) becomes\n",
        "equal to the defined size. In order to make all images have the same width and\n",
        "height, this class performs random crop such that all the images have the same\n",
        "width and height (in square format) before feeding the model. \n",
        "\n",
        "This cropper is also passed to HandKeypointDataset.\n",
        "'''\n",
        "class RandomCrop(object):\n",
        "    '''Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    '''\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, hand_keypoints, hand_box, label = sample['image'], sample['keypoints'], sample['boxes'], sample['labels']\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        # Decide cropping points concerning the hand box remains\n",
        "        # in the new image\n",
        "    \n",
        "        if (hand_box[0][0] <= 0):\n",
        "          hand_box[0][0] = 1\n",
        "        if (hand_box[0][1] <= 0):\n",
        "          hand_box[0][1] = 1\n",
        "        if (hand_box[0][2] >= w):\n",
        "          hand_box[0][2] = w - 1\n",
        "        if (hand_box[0][3] >= h):\n",
        "          hand_box[0][3] = h - 1\n",
        "\n",
        "        random_width_interval = (np.maximum(0, hand_box[0][2] - new_w), np.minimum(hand_box[0][0], w - new_w));\n",
        "        random_height_interval = (np.maximum(0, hand_box[0][3] - new_h), np.minimum(hand_box[0][1], h - new_h));\n",
        "        \n",
        "        top, left = 0, 0\n",
        "        if(random_height_interval[1] != 0):\n",
        "          top = np.random.randint(*random_height_interval) # new starting x coordinate\n",
        "        if(random_width_interval[1] != 0):\n",
        "          left = np.random.randint(*random_width_interval) # new starting y coordinate\n",
        "\n",
        "        image = image[top: top + new_h, left: left + new_w]\n",
        "\n",
        "        for i in range(len(hand_keypoints)):\n",
        "          hand_keypoints[i] = hand_keypoints[i] - [left, top, 0]\n",
        "        hand_box = hand_box - [left,  top,left, top]\n",
        "\n",
        "        return {'image': image, 'keypoints': hand_keypoints, 'boxes': hand_box, 'labels': label}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYaMc5phKEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, hand_keypoints, hand_box, label = sample['image'], sample['keypoints'], sample['boxes'], sample['labels']\n",
        "        # Swap color axis. \n",
        "        # The format is needed to feed the model.\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        \n",
        "        # normalize image\n",
        "        normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        image = normalize(image)\n",
        "       # image = normalizeImage(image)\n",
        "\n",
        "        return {'image': torch.from_numpy(image).cuda().type(torch.float32),\n",
        "                'keypoints': torch.from_numpy(hand_keypoints).cuda().type(torch.float32),\n",
        "                'boxes': torch.from_numpy(hand_box).cuda().type(torch.float32),\n",
        "                'labels': label.cuda().type(torch.int64)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV3IkMC1OrEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The function reads data from the directory and package the image name \n",
        "and other annotations. Output is like the following:\n",
        "\n",
        "[ ( image_file_name, {boxes,labels,keyponts} ) ]\n",
        "'''\n",
        "def get_hand_keypoints_frame(root_dir):\n",
        "    FRAME_SIZE = 1500; # Read this much data.\n",
        "\n",
        "    files = sorted([f for f in os.listdir(root_dir) if f.endswith('.json')])\n",
        "    hand_keypoints_frame = []\n",
        "    for f in files[:FRAME_SIZE]:\n",
        "        with open(root_dir+f, 'r') as fid:\n",
        "            dat = json.load(fid)\n",
        "\n",
        "        # Each file contains 1 hand annotation, with 21 points in\n",
        "        # 'hand_pts' of size 21x3, following this scheme:\n",
        "        # https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md#hand-output-format\n",
        "        # The 3rd column is 1 if valid.\n",
        "        \n",
        "        # Since we only try to detect hand keypoints rather than body keypoints etc.\n",
        "        # we have only one label here.\n",
        "        label = torch.Tensor([0]).view(1) \n",
        "\n",
        "        # Hand keypoints\n",
        "        hand_keypoints = np.array(dat['hand_pts'])\n",
        "        \n",
        "        # Hand box is not present in the dataset. Obtain it from the declared method above.\n",
        "        hand_box = get_hand_box(hand_keypoints)\n",
        "        \n",
        "        # Add image file name instead of the image itself.\n",
        "        dot_idx = f.rfind(\".\")\n",
        "        img_name = f[:dot_idx] + '.jpg'\n",
        "\n",
        "        # Since this is the ground truth, following statement makes no sense.\n",
        "        invalid = hand_keypoints[:,2]!=1\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = hand_box\n",
        "        target['labels'] = label\n",
        "        target['keypoints'] = hand_keypoints\n",
        "\n",
        "        hand_keypoints_frame.append((img_name, target))\n",
        "    return hand_keypoints_frame\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmQW1zEZ2Sp9",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAGyjM9BPEOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HandKeypointsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Hand Keypoints dataset defined in the doc. See the doc here:\n",
        "    https://pytorch.org/docs/stable/torchvision/models.html\n",
        "    \n",
        "    * boxes (FloatTensor[N, 4]): \n",
        "    the ground-truth boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W\n",
        "\n",
        "    * labels (Int64Tensor[N]): \n",
        "    the class label for each ground-truth box \n",
        "\n",
        "    * keypoints (FloatTensor[N, K, 3]): \n",
        "    the K keypoints location for each of the N instances, in the format [x, y, visibility],\n",
        "     where visibility=0 means that the keypoint is not visible\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        # hand_keypoints_frame = [ ( image_file_name, {boxes,labels,keyponts} ) ]\n",
        "        self.hand_keypoints_frame = get_hand_keypoints_frame(root_dir) # Contains the training data\n",
        "        \n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hand_keypoints_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.hand_keypoints_frame[idx][0])\n",
        "        image = io.imread(img_name) # Read image \n",
        "        \n",
        "        target = self.hand_keypoints_frame[idx][1]\n",
        "        \n",
        "        sample = {'image': image, 'keypoints': target['keypoints'], 'boxes': target['boxes'].reshape(1, -1), 'labels': torch.Tensor([0]).view(1)}\n",
        "        if self.transform:\n",
        "          sample = self.transform(sample)\n",
        "\n",
        "        # sample = {'image': sample['image'], 'target': {'keypoints': sample['keypoints'],  'boxes': sample['boxes'], 'labels': sample['labels']}}\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi70tXBl2W1y",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ-DdKnzNPgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True, num_keypoints=17)\n",
        "# print(\"Backbone Model:\\n\",model)\n",
        "for child in model.children():\n",
        "  for param in child.parameters():\n",
        "    param.requires_grad = True # keep these network parts as they are.\n",
        "\n",
        "for child in model.backbone.body.children():\n",
        "  for param in child.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for child in model.rpn.children():\n",
        "  for param in child.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# backbone fpn parameters need to be True \n",
        "# for child in model.backbone.fpn.children():\n",
        "#   for param in child.parameters():\n",
        "#     param.requires_grad = True\n",
        "# Define new predictor for 21 hand keypoints.\n",
        "\n",
        "model.roi_heads.keypoint_predictor = KeypointRCNNPredictor(512, 21) \n",
        "'''\n",
        "for child in model.roi_heads.keypoint_head.children():\n",
        "   for param in child.parameters():\n",
        "     param.requires_grad = False\n",
        "'''\n",
        "model.roi_heads.requires_grad = True\n",
        "model.roi_heads.keypoint_predictor.requires_grad = True\n",
        "\n",
        "#print(\"Modified model: \\n\", model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqZ2kRFH4xCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Train inside a funtion is required since runtime restriction on Google Colab\n",
        "'''\n",
        "def train(optimizer,epoch_loss,loader,model):\n",
        "  for idx_batch, sample_batched in enumerate(loader):\n",
        "    #torch.split(tensor, split_size_or_sections, dim=0)\n",
        "    #targets = [sample_batched['keypoints'], sample_batched['boxes'], sample_batched['labels']]\n",
        "    #we need to convert targets to list of dicts\n",
        "\n",
        "    targets = []\n",
        "    for i in range(sample_batched['keypoints'].size(0)):\n",
        "      temp = {'keypoints': sample_batched['keypoints'][i],\n",
        "              'boxes': sample_batched['boxes'][i],\n",
        "              'labels': sample_batched['labels'][i],\n",
        "              }\n",
        "      targets.append(temp)\n",
        "    y= model.forward(sample_batched['image'], targets)\n",
        "\n",
        "    total_loss = 0\n",
        "    for k, v in y.items():\n",
        "      # print(k, v.requires_grad)\n",
        "      total_loss += v\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    epoch_loss.append(total_loss)\n",
        "    optimizer.step()\n",
        "    if (idx_batch+1)%10 == 0:\n",
        "      pass\n",
        "      #print(\"Steps : {} Mean Loss: {}\".format(idx_batch+1, torch.mean(torch.stack(epoch_loss))))\n",
        "  return optimizer,epoch_loss,model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHOaWZx62bha",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFRyVi6TQ2yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "hand_label_dataset = HandKeypointsDataset(root_dir,transform=transforms.Compose([Rescale(310),RandomCrop(300),ToTensor()]))\n",
        "loader = DataLoader(hand_label_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akl4GTq_5aK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.cuda()\n",
        "model.train()\n",
        "# lr >= 0.02 gives nan epoch_loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.008,momentum=0.9, weight_decay=0.0005)\n",
        "epoch_loss = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcmFKVOa5f8B",
        "colab_type": "code",
        "outputId": "b4a937f9-e9de-4f7e-d80c-04680b1cc154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "optimizer, epoch_loss, model = train(optimizer, epoch_loss, loader, model)\n",
        "print(\"[Epoch 1] Mean Loss : \", torch.mean(torch.stack(epoch_loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Loss: 0.07997788488864899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qnN276g5ukH",
        "colab_type": "code",
        "outputId": "10bb0896-7ffc-4a92-8c4b-26f4d950fd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "optimizer, epoch_loss, model = train(optimizer, epoch_loss, loader, model)\n",
        "print(\"[Epoch 2] Mean Loss : \", torch.mean(torch.stack(epoch_loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 2] Mean Loss :  tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjQvfROF5vCx",
        "colab_type": "code",
        "outputId": "6af31d3f-2be3-4c8e-e3d9-92b835036636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "optimizer, epoch_loss, model = train(optimizer, epoch_loss, loader, model)\n",
        "print(\"[Epoch 3] Mean Loss : \", torch.mean(torch.stack(epoch_loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 3] Mean Loss :  tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLw4les45vSS",
        "colab_type": "code",
        "outputId": "2f23fdde-7dd0-4221-8d5c-f94fabc647ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "optimizer, epoch_loss, model = train(optimizer, epoch_loss, loader, model)\n",
        "print(\"[Epoch 4] Mean Loss : \", torch.mean(torch.stack(epoch_loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 4] Mean Loss :  tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-g6DsRo5vzd",
        "colab_type": "code",
        "outputId": "1c8993fb-1374-4202-9526-3782c14851fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "optimizer, epoch_loss, model = train(optimizer, epoch_loss, loader, model)\n",
        "print(\"[Epoch 5] Mean Loss : \", torch.mean(torch.stack(epoch_loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 5] Mean Loss :  tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnwA3Vi9xGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'model-epoch5-size300-lr0_05')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM0R2Mp68CNU",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD4bP_vURQr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "test_image = io.imread(drive_base_path + \"/hand_labels/manual_train/Jackie_unit_10.flv_000139_r.jpg\") # Read image \n",
        "test_image = test_image / 255.\n",
        "test_image = test_image -np.array([0.485, 0.456, 0.406])[None,None,:]\n",
        "test_image = test_image / np.array([0.229, 0.224, 0.225])[None,None,:]\n",
        "test_image = torch.from_numpy(test_image.transpose((2, 0, 1))).cuda().type(torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qmln1JsIsDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = model([test_image])\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}